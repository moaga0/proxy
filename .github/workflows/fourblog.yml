name: fourblog-crawl

on:
  workflow_dispatch:
    inputs:
      maxPage:
        required: true
        default: "10"

jobs:
  crawl:
    runs-on: ubuntu-latest

    env:
      PROXY_LIST: |
        "socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST }}:${{ secrets.PROXY_PORT }}"
        "socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST_2 }}:${{ secrets.PROXY_PORT_2 }}"
        "socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST_3 }}:${{ secrets.PROXY_PORT_3 }}"

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Prepare workspace
        run: |
          mkdir -p tmp/visit tmp/shipping
          echo '{"visit":{"pages":[]}, "shipping":{"pages":[]}}' > result.json

      - name: Prepare proxy rotation + UA + backoff script
        run: |
          cat << 'EOF' > crawler_utils.sh
          #!/bin/bash

          # ======== PROXY ROTATION ========
          mapfile -t RAW_PROXIES <<< "$PROXY_LIST"

          PROXIES=()
          for p in "${RAW_PROXIES[@]}"; do
            CLEAN=$(echo "$p" | sed 's/^"//;s/"$//')
            if [ -n "$CLEAN" ]; then
              PROXIES+=("$CLEAN")
            fi
          done

          IDX_FILE="/tmp/proxy_idx"
          BAD_PROXIES_FILE="/tmp/bad_proxies"

          touch "$BAD_PROXIES_FILE"

          if [ ! -f "$IDX_FILE" ]; then
            echo 0 > "$IDX_FILE"
          fi

          get_next_proxy() {
            local start_idx=$(cat "$IDX_FILE")
            local idx=$start_idx

            for i in "${!PROXIES[@]}"; do
              local p="${PROXIES[$idx]}"

              if ! grep -q "$p" "$BAD_PROXIES_FILE"; then
                echo "$p"
                echo $(( (idx + 1) % ${#PROXIES[@]} )) > "$IDX_FILE"
                return
              fi

              idx=$(( (idx + 1) % ${#PROXIES[@]} ))
            done

            echo "ERROR_NO_PROXY"
          }

          mark_bad_proxy() {
            echo "$1" >> "$BAD_PROXIES_FILE"
          }

          # ======== RANDOM USER-AGENT ========
          get_random_ua() {
            UA_LIST=(
              "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"
              "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/537.36 Chrome/119.0.0.0 Safari/537.36"
              "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Edg/120.0.0.0"
              "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/605.1.15 Version/17.0 Safari/605.1.15"
            )

            echo "${UA_LIST[$((RANDOM % ${#UA_LIST[@]}))]}"
          }

          # ======== BACKOFF ON 429 ========
          backoff_sleep() {
            local base=$(( RANDOM % 10 + 10 )) # 10~19Ï¥à
            echo "‚è≥ Backoff sleep: ${base}s"
            sleep $base
          }

          EOF

          chmod +x crawler_utils.sh

      - name: Debug initial proxy
        run: |
          PROXY_URL=$(./crawler_utils.sh get_next_proxy)
          echo "First PROXY_URL=$PROXY_URL"
          curl -x "$PROXY_URL" https://ipinfo.io || echo "‚ö†Ô∏è PROXY TEST FAILED"

      # ======================
      # VISIT (local)
      # ======================
      - name: Crawl FourBlog VISIT (local)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching VISIT page $p (offset=$OFFSET)"

            RETRY=0
            MAX_RETRY=3

            while [ $RETRY -lt $MAX_RETRY ]; do
              PROXY_URL=$(./crawler_utils.sh get_next_proxy)
              UA=$(./crawler_utils.sh get_random_ua)

              echo "üîÅ Using proxy: $PROXY_URL"
              echo "üåê UA: $UA"

              URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=local&location=&location1=&search=&bid="

              RES=$(curl -s -x "$PROXY_URL" \
                -w "\nHTTP_STATUS:%{http_code}" \
                -H "User-Agent: $UA" \
                "$URL" || true)

              STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')
              BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')

              echo "‚û°Ô∏è HTTP Status: $STATUS"

              if [ "$STATUS" = "200" ]; then
                break
              elif [ "$STATUS" = "429" ]; then
                ./crawler_utils.sh backoff_sleep
              else
                echo "‚ö†Ô∏è Bad proxy, rotating..."
                ./crawler_utils.sh mark_bad_proxy "$PROXY_URL"
              fi

              RETRY=$((RETRY+1))
            done

            if [ "$STATUS" != "200" ]; then
              echo "üõë Give up this page after retries"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/visit_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
              '.visit.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            SLEEP_SEC=$(( RANDOM % 3 + 3 ))
            echo "üò¥ Sleep ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      # ======================
      # SHIPPING (deliv)
      # ======================
      - name: Crawl FourBlog SHIPPING (deliv)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching SHIPPING page $p (offset=$OFFSET)"

            RETRY=0
            MAX_RETRY=3

            while [ $RETRY -lt $MAX_RETRY ]; do
              PROXY_URL=$(./crawler_utils.sh get_next_proxy)
              UA=$(./crawler_utils.sh get_random_ua)

              echo "üîÅ Using proxy: $PROXY_URL"
              echo "üåê UA: $UA"

              URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=deliv&location=&location1=&search=&bid="

              RES=$(curl -s -x "$PROXY_URL" \
                -w "\nHTTP_STATUS:%{http_code}" \
                -H "User-Agent: $UA" \
                "$URL" || true)

              STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')
              BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')

              echo "‚û°Ô∏è HTTP Status: $STATUS"

              if [ "$STATUS" = "200" ]; then
                break
              elif [ "$STATUS" = "429" ]; then
                ./crawler_utils.sh backoff_sleep
              else
                echo "‚ö†Ô∏è Bad proxy, rotating..."
                ./crawler_utils.sh mark_bad_proxy "$PROXY_URL"
              fi

              RETRY=$((RETRY+1))
            done

            if [ "$STATUS" != "200" ]; then
              echo "üõë Give up this page after retries"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/shipping_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/shipping_page_${p}.json \
              '.shipping.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            SLEEP_SEC=$(( RANDOM % 3 + 3 ))
            echo "üò¥ Sleep ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      - name: Upload artifact (JSON)
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result-json
          path: result.json
          if-no-files-found: error
