name: fourblog-crawl

on:
  workflow_dispatch:
    inputs:
      maxPage:
        required: true
        default: "10"

jobs:
  crawl:
    runs-on: ubuntu-latest

    env:
      PROXY_LIST: |
        socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST }}:${{ secrets.PROXY_PORT }}
        socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST_2 }}:${{ secrets.PROXY_PORT_2 }}
        socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST_3 }}:${{ secrets.PROXY_PORT_3 }}

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Prepare workspace
        run: |
          mkdir -p tmp/visit tmp/shipping
          echo '{"visit":{"pages":[]}, "shipping":{"pages":[]}}' > result.json

      # ============================
      # üîπ Îã® 1Ìöå: ÌîÑÎ°ùÏãú 1Í∞ú ÏÑ†ÌÉù
      # ============================
      - name: Select SINGLE proxy (fixed for whole job)
        run: |
          PROXY_URL=$(echo "$PROXY_LIST" | shuf -n 1)
          echo "SELECTED_PROXY=$PROXY_URL" >> $GITHUB_ENV
          echo "‚úÖ Selected proxy: $PROXY_URL"

      # ============================
      # üîπ Îã® 1Ìöå: User-Agent ÏÑ†ÌÉù
      # ============================
      - name: Select SINGLE User-Agent (fixed)
        run: |
          UA_LIST=(
            "Mozilla/5.0 (Windows NT 10.0; Win64) AppleWebKit/537.36 (Chrome/120.0.0.0 Safari/537.36)"
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 (Safari/17.0)"
            "Mozilla/5.0 (Windows NT 10.0; Win64) AppleWebKit/537.36 (Edg/120.0.0.0)"
          )

          UA=${UA_LIST[$RANDOM % ${#UA_LIST[@]}]}
          echo "SELECTED_UA=$UA" >> $GITHUB_ENV
          echo "‚úÖ Selected User-Agent: $UA"

      - name: Debug Proxy (ÌïÑÏàò Ï≤¥ÌÅ¨)
        run: |
          echo "Using PROXY=$SELECTED_PROXY"
          curl -x "$SELECTED_PROXY" https://ipinfo.io || echo "‚ö†Ô∏è PROXY TEST FAILED"

      # ======================
      # VISIT (local)
      # ======================
      - name: Crawl FourBlog VISIT (local)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching VISIT page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=local&location=&location1=&search=&bid="

            RES=$(curl -s -x "$SELECTED_PROXY" \
              -w "\nHTTP_STATUS:%{http_code}" \
              -H "User-Agent: $SELECTED_UA" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "‚û°Ô∏è HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "üõë Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/visit_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
              '.visit.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            # ‚úÖ ÎûúÎç§ ÎåÄÍ∏∞ (3~5Ï¥à)
            SLEEP_SEC=$(( RANDOM % 3 + 3 ))
            echo "üò¥ Sleep ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      # ======================
      # SHIPPING (deliv)
      # ======================
      - name: Crawl FourBlog SHIPPING (deliv)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching SHIPPING page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=deliv&location=&location1=&search=&bid="

            RES=$(curl -s -x "$SELECTED_PROXY" \
              -w "\nHTTP_STATUS:%{http_code}" \
              -H "User-Agent: $SELECTED_UA" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "‚û°Ô∏è HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "üõë Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/shipping_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/shipping_page_${p}.json \
              '.shipping.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            # ‚úÖ ÎûúÎç§ ÎåÄÍ∏∞ (3~5Ï¥à)
            SLEEP_SEC=$(( RANDOM % 3 + 3 ))
            echo "üò¥ Sleep ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      # ======================
      # JSON Í∑∏ÎåÄÎ°ú ÏóÖÎ°úÎìú
      # ======================
      - name: Upload artifact (JSON)
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result-json
          path: result.json
          if-no-files-found: error
