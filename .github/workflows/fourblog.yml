name: fourblog-crawler

on:
  workflow_dispatch:
    inputs:
      maxPage:
        required: true
        default: "10"

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      PROXY_USER: ${{ secrets.PROXY_USER }}
      PROXY_PASS: ${{ secrets.PROXY_PASS }}
      PROXY_HOST: ${{ secrets.PROXY_HOST }}
      PROXY_URL: "http://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST }}:80"

    steps:
      - name: Checkout (dummy)
        run: echo "No repo needed, just crawling"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Initialize result.json
        run: |
          echo '{
            "visit": { "pages": [] }
          }' > result.json

      - name: Prepare temp folders
        run: |
          mkdir -p tmp/visit

      - name: Crawl Fourblog VISIT Trials (With Proxy + Retry)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            echo "ðŸš€ Fetching VISIT page $p"

            # í”„ë¡ì‹œë¡œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 3íšŒ ìž¬ì‹œë„)
            for i in {1..3}; do
              RES=$(curl -s -x "$PROXY_URL" \
                -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64)" \
                -H "Referer: https://www.fourblog.co.kr/" \
                "https://4blog.net/loadMoreDataCategory?offset=$p&limit=30&category=all&category1=local&location=&location1=&search=&bid=")

              COUNT=$(echo "$RES" | jq '.data.data | length' 2>/dev/null || echo 0)

              if [ "$COUNT" -gt 0 ]; then
                break
              fi

              echo "âš ï¸ Retry $i/3 for page $p (possible IP block)"
              sleep 5
            done

            if [ "$COUNT" -eq 0 ]; then
              echo "ðŸ›‘ No more data. Stop crawling."
              break
            fi

            echo "$RES" | jq -r '.data.data[].wr_id' | \
              xargs -n1 -P20 -I{} sh -c "
                curl -s -x \"$PROXY_URL\" \
                  -H 'User-Agent: Mozilla/5.0' \
                  -H 'Referer: https://www.fourblog.co.kr/' \
                  'https://api.fourblog.co.kr/v1/trial/detail?wr_id={}' > tmp/visit/{}.json
              "

            jq -s '.' tmp/visit/*.json > tmp/visit_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
              '.visit.pages += [{\"page\": ($p|tonumber), \"data\": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json
            rm tmp/visit/*.json
          done

      - name: Upload result.json
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result
          path: result.json
