name: fourblog-crawl

on:
  workflow_dispatch:
    inputs:
      maxPage:
        required: true
        default: "10"

jobs:
  crawl:
    runs-on: ubuntu-latest

    env:
      PROXY_URL: "socks5h://${{ secrets.PROXY_USER }}:${{ secrets.PROXY_PASS }}@${{ secrets.PROXY_HOST }}:${{ secrets.PROXY_PORT }}"

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Prepare workspace
        run: |
          mkdir -p tmp/visit tmp/shipping
          echo '{"visit":{"pages":[]}, "shipping":{"pages":[]}}' > result.json

      - name: Debug Proxy (ÌïÑÏàò Ï≤¥ÌÅ¨)
        run: |
          echo "PROXY_URL=$PROXY_URL"
          curl -x "$PROXY_URL" https://ipinfo.io || echo "‚ö†Ô∏è PROXY TEST FAILED"

      # ======================
      # VISIT (local)
      # ======================
      - name: Crawl FourBlog VISIT (local)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching VISIT page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=local&location=&location1=&search=&bid="

            RES=$(curl -s -x "$PROXY_URL" \
              -w "\nHTTP_STATUS:%{http_code}" \
              -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64)" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "‚û°Ô∏è HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "üõë Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/visit_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
              '.visit.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json
          done

      # ======================
      # SHIPPING (deliv)
      # ======================
      - name: Crawl FourBlog SHIPPING (deliv)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "üöÄ Fetching SHIPPING page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=deliv&location=&location1=&search=&bid="

            RES=$(curl -s -x "$PROXY_URL" \
              -w "\nHTTP_STATUS:%{http_code}" \
              -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64)" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "‚û°Ô∏è HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "üõë Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "‚õî No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/shipping_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/shipping_page_${p}.json \
              '.shipping.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json
          done

      # ======================
      # JSON Í∑∏ÎåÄÎ°ú ÏóÖÎ°úÎìú
      # ======================
      - name: Upload artifact (JSON)
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result-json
          path: result.json
          if-no-files-found: error
