name: fourblog-crawl

on:
  workflow_dispatch:
    inputs:
      maxPage:
        required: true
        default: "10"

jobs:
  crawl:
    runs-on: ubuntu-latest

    env:
      # ====== í”„ë¡ì‹œ í’€ (íšŒì „ìš©) ======
      PROXY_LIST: |
        socks5h://${{ secrets.PROXY_USER_1 }}:${{ secrets.PROXY_PASS_1 }}@${{ secrets.PROXY_HOST_1 }}:${{ secrets.PROXY_PORT_1 }}
        socks5h://${{ secrets.PROXY_USER_2 }}:${{ secrets.PROXY_PASS_2 }}@${{ secrets.PROXY_HOST_2 }}:${{ secrets.PROXY_PORT_2 }}
        socks5h://${{ secrets.PROXY_USER_3 }}:${{ secrets.PROXY_PASS_3 }}@${{ secrets.PROXY_HOST_3 }}:${{ secrets.PROXY_PORT_3 }}

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Prepare workspace
        run: |
          mkdir -p tmp/visit tmp/shipping
          echo '{"visit":{"pages":[]}, "shipping":{"pages":[]}}' > result.json

      # ============================
      # ì„¸ì…˜ ë‹¨ìœ„ UA + í”„ë¡ì‹œ ì„ ì •
      # ============================
      - name: Select Session UA & Proxy
        id: session
        run: |
          # ëœë¤ User-Agent (ì„¸ì…˜ë‹¹ 1íšŒ)
          SELECTED_UA=$(./crawler_utils.sh get_random_ua)

          # í”„ë¡ì‹œ í’€ì—ì„œ ëœë¤ 1ê°œ ì„ íƒ
          SELECTED_PROXY=$(echo "$PROXY_LIST" | shuf -n 1 | tr -d '[:space:]')

          echo "SELECTED_UA=$SELECTED_UA" >> $GITHUB_OUTPUT
          echo "SELECTED_PROXY=$SELECTED_PROXY" >> $GITHUB_OUTPUT

          echo "ğŸ§‘â€ğŸ’» Session UA: $SELECTED_UA"
          echo "ğŸŒ Session Proxy: $SELECTED_PROXY"

      - name: Debug Proxy (í•„ìˆ˜ ì²´í¬)
        run: |
          echo "Testing proxy: ${{ steps.session.outputs.SELECTED_PROXY }}"
          curl -x "${{ steps.session.outputs.SELECTED_PROXY }}" https://ipinfo.io || echo "âš ï¸ PROXY TEST FAILED"

      # ======================
      # VISIT (local)
      # ======================
      - name: Crawl FourBlog VISIT (local)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "ğŸš€ Fetching VISIT page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=local&location=&location1=&search=&bid="

            RES=$(curl -s \
              -x "${{ steps.session.outputs.SELECTED_PROXY }}" \
              -A "${{ steps.session.outputs.SELECTED_UA }}" \
              -w "\nHTTP_STATUS:%{http_code}" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "â¡ï¸ HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "ğŸ›‘ Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "â›” No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/visit_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
              '.visit.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            # âœ… ì‚¬ëŒì²˜ëŸ¼ 3~5ì´ˆ ëœë¤ ëŒ€ê¸°
            SLEEP_SEC=$((RANDOM % 3 + 3))
            echo "ğŸ˜´ Sleeping ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      # ======================
      # SHIPPING (deliv)
      # ======================
      - name: Crawl FourBlog SHIPPING (deliv)
        run: |
          set -e

          for ((p=1; p<=${{ inputs.maxPage }}; p++)); do
            OFFSET=$(( (p - 1) * 30 ))
            echo "ğŸš€ Fetching SHIPPING page $p (offset=$OFFSET)"

            URL="https://4blog.net/loadMoreDataCategory?offset=$OFFSET&limit=30&category=all&category1=deliv&location=&location1=&search=&bid="

            RES=$(curl -s \
              -x "${{ steps.session.outputs.SELECTED_PROXY }}" \
              -A "${{ steps.session.outputs.SELECTED_UA }}" \
              -w "\nHTTP_STATUS:%{http_code}" \
              "$URL" || true)

            BODY=$(echo "$RES" | sed -n '1,/HTTP_STATUS:/p' | sed '$d')
            STATUS=$(echo "$RES" | grep "HTTP_STATUS:" | awk -F: '{print $2}')

            echo "â¡ï¸ HTTP Status: $STATUS"

            if [ "$STATUS" != "200" ]; then
              echo "ğŸ›‘ Stop crawling (status != 200)"
              break
            fi

            COUNT=$(echo "$BODY" | jq 'length' 2>/dev/null || echo 0)
            if [ "$COUNT" -eq 0 ]; then
              echo "â›” No more data. Stop."
              break
            fi

            echo "$BODY" > tmp/shipping_page_${p}.json

            jq --arg p "$p" --slurpfile pageData tmp/shipping_page_${p}.json \
              '.shipping.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json

            mv tmp.json result.json

            # âœ… ë™ì¼í•œ ëŒ€ê¸° ì „ëµ ì ìš©
            SLEEP_SEC=$((RANDOM % 3 + 3))
            echo "ğŸ˜´ Sleeping ${SLEEP_SEC}s..."
            sleep $SLEEP_SEC
          done

      # ======================
      # JSON ê·¸ëŒ€ë¡œ ì—…ë¡œë“œ
      # ======================
      - name: Upload artifact (JSON)
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result-json
          path: result.json
          if-no-files-found: error
