name: fourblog-crawler

on:
  workflow_dispatch:
    inputs:
      visitMaxPage:
        required: true
        default: "5"
      shippingMaxPage:
        required: true
        default: "20"

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl

      - name: Initialize result.json
        run: |
          echo '{
            "visit": { "pages": [] },
            "shipping": { "pages": [] }
          }' > result.json

      - name: Prepare temp folders
        run: |
          mkdir -p tmp/visit tmp/shipping

      # ============================
      # Webshare 프록시 환경 변수 세팅
      # ============================
      - name: Set Webshare Proxy
        env:
          http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80: "${{ secrets.http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80 }}"
        run: |
          echo "Using proxy: $http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80"
          echo "http_proxy=$http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80" >> $GITHUB_ENV
          echo "https_proxy=$http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80" >> $GITHUB_ENV

      # ============================
      # 방문형 (포블로그 유사 구조 가정)
      # ============================
      - name: Crawl Visit Trials (Parallel)
        run: |
          for ((p=1; p<=${{ inputs.visitMaxPage }}; p++)); do
            echo "Fetching Visit Trials Page $p"

            RES=$(curl -s --retry 3 --retry-delay 2 -x "$http://$PROXY_USER:$PROXY_PASS@$PROXY_HOST:80" -H "User-Agent: Mozilla/5.0" \
              "https://api.fourblog.co.kr/v1/trial?order=latest&page=$p&count=50&type=VISIT")

            COUNT=$(echo "$RES" | jq '.data.list | length')
            if [ "$COUNT" -eq 0 ]; then
              echo "No more Visit Trials, stopping."
              break
            fi

            # 병렬 상세조회 (최대 20개)
            echo "$RES" | jq -r '.data.list[].id' | xargs -n1 -P10 -I{} sh -c \
              "curl -s -H 'User-Agent: Mozilla/5.0' \
               'https://api.fourblog.co.kr/v1/trial/detail?id={}' > tmp/visit/{}.json"

            # 페이지 단위 병합
            jq -s '.' tmp/visit/*.json > tmp/visit_page_${p}.json
            jq --arg p "$p" --slurpfile pageData tmp/visit_page_${p}.json \
               '.visit.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json
            mv tmp.json result.json
            rm tmp/visit/*.json
          done

      # ============================
      # 배송형 (포블로그 유사 구조 가정)
      # ============================
      - name: Crawl Shipping Trials (Parallel)
        run: |
          for ((p=1; p<=${{ inputs.shippingMaxPage }}; p++)); do
            echo "Fetching Shipping Trials Page $p"

            RES=$(curl -s -H "User-Agent: Mozilla/5.0" \
              "https://api.fourblog.co.kr/v1/trial?order=latest&page=$p&count=50&type=SHIPPING")

            COUNT=$(echo "$RES" | jq '.data.list | length')
            if [ "$COUNT" -eq 0 ]; then
              echo "No more Shipping Trials, stopping."
              break
            fi

            echo "$RES" | jq -r '.data.list[].id' | xargs -n1 -P10 -I{} sh -c \
              "curl -s -H 'User-Agent: Mozilla/5.0' \
               'https://api.fourblog.co.kr/v1/trial/detail?id={}' > tmp/shipping/{}.json"

            jq -s '.' tmp/shipping/*.json > tmp/shipping_page_${p}.json
            jq --arg p "$p" --slurpfile pageData tmp/shipping_page_${p}.json \
               '.shipping.pages += [{"page": ($p|tonumber), "data": $pageData[0]}]' result.json > tmp.json
            mv tmp.json result.json
            rm tmp/shipping/*.json
          done

      - name: Upload result.json
        uses: actions/upload-artifact@v4
        with:
          name: fourblog-result
          path: result.json
